For the API exercise I built a conversational support assistant called a Support Copilot. It generates email responses to customers who have emailed the help center of Olivetto, a (fictional) olive oil company. The repository contains a folder of example customer emails as well as a small knowledge base of documents about the company's policies generated with ChatGPT.

The workflow follows the following steps:
1. A customer sends an email   
2. First LLM call to OpenAI drafts an initial reply draft, using the knowledge base for grounded responses. This returns a structured output containing the email draft, any clarifying questions for the customer, and all relevant details from the knowledge base used in generating the response.
3. Second LLM call to Anthropic for review and critique. It checks the draft email for any hallucinations, poor grounding, or tone issues (in this example too cold, too verbose, overly apologetic). It outputs structured feedback and a verdict regarding the email - revise, needs human, OK.
4. Final LLM call, again to OpenAI, writes a final customer-ready email. It incorporates the original draft and reviewer feedback, and only returns the text of the email.

I chose to split generation and review across different providers. In my experience using a different model for review helps to reduce shared failures and increases the odds of catching errors that the original model is likely to miss. If the same model is used to review its own output, it can be more likely to reinforce its original assumptions or make the same hallucinations. However, a second model approaches the task with different training data and so is more likely to question unsupported claims or tone issues. In this customer-support scenario, a hallucination could cause legitimate issues (such as promising a refund against company policy).  

In practice, the initial draft had an appropriate tone and was well-grounded in general but would make small assumptions that weren't fully supported by the knowledge base. In one run (included in screenshots and also stored in example_output), the draft promised to send the customer "refund/return instructions" despite the knowledge base not specifying a physical return process for damaged items. The reviewer model caught these subtle issues, flagged the unsupported claim, and suggested softening the tone to avoid over-promising. It also noted the inclusion of unnecessary clarifying questions which made the response appear overly bureaucratic. Incorporating this feedback into the rewrite resulted in a final email that was more policy-aligned and customer-friendly, with a more empathetic tone.

The API made it easy to swap out LLM providers without reworking prompt structure or response parsing, which simplified testing different models for generation versus review.  Changing other parameters (model selection, temperature, etc) was similarly straightforward.  I did not build a full test harness for parameter tuning, but systematically evaluating these settings is something I would prioritize if this assistant were being productionized, especially to balance response quality, latency, and cost.

Because embeddings aren't supported in the API yet I passed the entire knowledge base as a long context rather than retrieving smaller chunks.  This approach is obviously less than ideal, but is workable for the small toy knowledge base I generated for this project.

For this scoped project, I honestly didn't run into any blockers or bugs using the API. The biggest tradeoff is the lack of embeddings and my decision to use a long context, which increases token usage (cost and latency) and may make it difficult to reference relevant policy details if the context grows too long.  In production, I would utilize an external retrieval system until embeddings become available via Concentrate, but for this short exercise the long-context approach was workable.